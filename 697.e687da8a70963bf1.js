"use strict";(self.webpackChunkllm_agent_hub=self.webpackChunkllm_agent_hub||[]).push([[697],{697:(w,c,i)=>{i.r(c),i.d(c,{BibliographyComponent:()=>R});var s=i(177),g=i(401),e=i(438),u=i(260);const d=(n,a)=>({"bg-blue-600 text-white":n,"bg-gray-100 text-gray-700 hover:bg-gray-200 dark:bg-gray-700 dark:text-gray-300 dark:hover:bg-gray-600":a}),f=(n,a,t,r)=>({"bg-green-100 text-green-800 dark:bg-green-900/20 dark:text-green-400":n,"bg-purple-100 text-purple-800 dark:bg-purple-900/20 dark:text-purple-400":a,"bg-orange-100 text-orange-800 dark:bg-orange-900/20 dark:text-orange-400":t,"bg-gray-100 text-gray-800 dark:bg-gray-900/20 dark:text-gray-400":r});function p(n,a){if(1&n){const t=e.RV6();e.j41(0,"button",58),e.bIt("click",function(){const o=e.eBV(t).$implicit,l=e.XpG();return e.Njj(l.selectCategory(o.key))}),e.nrm(1,"i"),e.EFF(2),e.k0s()}if(2&n){const t=a.$implicit,r=e.XpG();e.Y8G("ngClass",e.l_i(5,d,r.selectedCategory===t.key,r.selectedCategory!==t.key)),e.R7$(),e.HbH(t.icon+" mr-2"),e.R7$(),e.Lme(" ",t.name," (",t.count,") ")}}function m(n,a){if(1&n){const t=e.RV6();e.j41(0,"button",44),e.bIt("click",function(){e.eBV(t);const o=e.XpG();return e.Njj(o.selectCategory("all"))}),e.nrm(1,"i",59),e.EFF(2," Clear Filter "),e.k0s()}}function b(n,a){if(1&n&&(e.j41(0,"span",72),e.EFF(1),e.k0s()),2&n){const t=e.XpG().$implicit;e.R7$(),e.JRh(t.venue)}}function h(n,a){if(1&n&&(e.j41(0,"span",73),e.EFF(1),e.k0s()),2&n){const t=e.XpG().$implicit;e.R7$(),e.SpI("vol. ",t.volume,"")}}function y(n,a){if(1&n&&(e.j41(0,"span",73),e.EFF(1),e.k0s()),2&n){const t=e.XpG().$implicit;e.R7$(),e.SpI("pp. ",t.pages,"")}}function v(n,a){if(1&n&&(e.j41(0,"span",73),e.EFF(1),e.k0s()),2&n){const t=e.XpG().$implicit;e.R7$(),e.JRh(t.year)}}function k(n,a){if(1&n&&(e.j41(0,"span",73),e.EFF(1),e.k0s()),2&n){const t=e.XpG().$implicit;e.R7$(),e.SpI("DOI: ",t.doi,"")}}function x(n,a){if(1&n&&(e.j41(0,"div",74)(1,"a",75),e.nrm(2,"i",76),e.EFF(3," View Source "),e.k0s()()),2&n){const t=e.XpG().$implicit;e.R7$(),e.Y8G("href",t.url,e.B4B)}}function F(n,a){if(1&n&&(e.j41(0,"div",77),e.EFF(1),e.k0s()),2&n){const t=e.XpG().$implicit;e.R7$(),e.SpI(" Accessed: ",t.accessDate," ")}}function _(n,a){if(1&n&&(e.j41(0,"div",60)(1,"div",61)(2,"span",62),e.EFF(3),e.k0s(),e.j41(4,"span",63),e.EFF(5),e.k0s()(),e.j41(6,"div",64)(7,"span",65),e.EFF(8),e.k0s()(),e.j41(9,"div",64)(10,"span",66),e.EFF(11),e.k0s()(),e.j41(12,"div",67),e.DNE(13,b,2,1,"span",68)(14,h,2,1,"span",69)(15,y,2,1,"span",69)(16,v,2,1,"span",69)(17,k,2,1,"span",69),e.k0s(),e.DNE(18,x,4,1,"div",70)(19,F,2,1,"div",71),e.k0s()),2&n){const t=a.$implicit,r=e.XpG();e.R7$(3),e.SpI(" [",t.number,"] "),e.R7$(),e.Y8G("ngClass",e.ziG(12,f,"journal"===t.type,"conference"===t.type,"web"===t.type,"book"===t.type)),e.R7$(),e.SpI(" ",r.getCategoryName(t.type)," "),e.R7$(3),e.JRh(t.authors),e.R7$(3),e.SpI('"',t.title,'"'),e.R7$(2),e.Y8G("ngIf",t.venue),e.R7$(),e.Y8G("ngIf",t.volume),e.R7$(),e.Y8G("ngIf",t.pages),e.R7$(),e.Y8G("ngIf",t.year),e.R7$(),e.Y8G("ngIf",t.doi),e.R7$(),e.Y8G("ngIf",t.url),e.R7$(),e.Y8G("ngIf",t.accessDate)}}let R=(()=>{class n{constructor(t){this.navigationService=t,this.selectedCategory="all",this.categories=[{key:"all",name:"All References",icon:"fas fa-list",count:35},{key:"journal",name:"Journal Articles",icon:"fas fa-file-alt",count:8},{key:"conference",name:"Conference Papers",icon:"fas fa-users",count:12},{key:"web",name:"Web Resources",icon:"fas fa-globe",count:10},{key:"book",name:"Books",icon:"fas fa-book",count:5}],this.references=[{number:1,authors:"R. S. Sutton and A. Barto",title:"Reinforcement learning: An introduction",venue:"The MIT Press",year:"2020",type:"book",url:""},{number:2,authors:"N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao",title:"Reflexion: Language Agents with Verbal Reinforcement Learning",year:"Mar. 2023",type:"conference",url:"http://arxiv.org/pdf/2303.11366v4"},{number:3,authors:"Z. Yang et al.",title:"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",year:"Sep. 2018",type:"conference",url:"http://arxiv.org/pdf/1809.09600v1"},{number:4,authors:"M. Shridhar, X. Yuan, M.-A. C\xf4t\xe9, Y. Bisk, A. Trischler, and M. Hausknecht",title:"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",year:"Oct. 2020",type:"conference",url:"http://arxiv.org/pdf/2010.03768v2"},{number:5,authors:"M. Renze and E. Guven",title:"Self-Reflection in LLM Agents: Effects on Problem-Solving Performance",venue:"vol. 35",pages:"476\u2013483",year:"2024",doi:"10.1109/FLLM63129.2024.10852493",type:"journal"},{number:6,authors:"K. Se",title:"How Do Agents Learn from Their Own Mistakes? The Role of Reflection in AI",url:"https://www.turingpost.com/p/reflection",accessDate:"Jun. 17 2025",type:"web"},{number:7,authors:"A. Madaan et al.",title:"Self-Refine: Iterative Refinement with Self-Feedback",year:"Mar. 2023",type:"conference",url:"http://arxiv.org/pdf/2303.17651v2"},{number:8,authors:"S. Yao et al.",title:"ReAct: Synergizing Reasoning and Acting in Language Models",year:"Oct. 2022",type:"conference",url:"http://arxiv.org/pdf/2210.03629v3"},{number:9,authors:"T. Schick et al.",title:"Toolformer: Language Models Can Teach Themselves to Use Tools",venue:"Advances in Neural Information Processing Systems",volume:"36",year:"2023",type:"conference",url:"https://arxiv.org/abs/2302.04761",accessDate:"Jun. 17 2025"},{number:10,authors:"G. Wang et al.",title:"Voyager: An Open-Ended Embodied Agent with Large Language Models",year:"May. 2023",type:"conference",url:"http://arxiv.org/pdf/2305.16291v2"},{number:11,authors:"LangChain",title:"Reflection Agents",url:"https://blog.langchain.dev/reflection-agents/",accessDate:"Jun. 17 2025",type:"web"},{number:12,authors:"DAIR.AI",title:"Reflexion",url:"https://www.promptingguide.ai/techniques/reflexion",accessDate:"Jun. 17 2025",type:"web"},{number:13,authors:"S. Schulhoff et al.",title:"The Prompt Report: A Systematic Survey of Prompt Engineering Techniques",year:"Jun. 2024",type:"conference",url:"http://arxiv.org/pdf/2406.06608v6"},{number:14,authors:"J. Kaur",title:"Reflection Agent Prompting: Strategies for More Efficient Performance",url:"https://www.akira.ai/blog/reflection-agent-prompting",accessDate:"Jun. 17 2025",type:"web"},{number:15,authors:"DeepSeek-AI et al.",title:"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",year:"Jan. 2025",type:"conference",url:"http://arxiv.org/pdf/2501.12948v1"},{number:16,authors:"OpenAI",title:"GPT-4o System Card: Technical report",venue:"OpenAI, San Francisco, CA",year:"May. 2024",url:"https://openai.com/research/gpt-4o-system-card",accessDate:"Jun. 17 2025",type:"web"},{number:17,authors:"Daniel Dominguez",title:"Minecraft Welcomes Its First LLM-Powered Agent",venue:"InfoQ",year:"2023",url:"https://www.infoq.com/news/2023/05/minecraft-voyager-llm-agent/",type:"web"},{number:18,authors:"Ben Wang and Aran Komatsuzaki",title:"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",year:"2021",url:"https://github.com/kingoflolz/mesh-transformer-jax",type:"web"},{number:19,authors:"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al.",title:"Language models are few-shot learners",venue:"Advances in Neural Information Processing Systems",volume:"33",pages:"1877\u20131901",year:"2020",url:"https://arxiv.org/pdf/2005.14165",accessDate:"Jun. 19 2025",type:"conference"},{number:20,authors:"Q. Dong et al.",title:"A Survey on In-context Learning",year:"Dec. 2022",type:"conference",url:"https://arxiv.org/abs/2301.00234"},{number:21,authors:"J. Uesato et al.",title:"Solving math word problems with process- and outcome-based feedback",year:"Nov. 2022",type:"conference",url:"http://arxiv.org/pdf/2211.14275v1"},{number:22,authors:"H. Lightman et al.",title:"Let's Verify Step by Step",year:"May. 2023",type:"conference",url:"http://arxiv.org/pdf/2305.20050v1"},{number:23,authors:"N. Carlini et al.",title:"Extracting Training Data from Large Language Models",year:"Dec. 2020",type:"conference",url:"http://arxiv.org/pdf/2012.07805v2"},{number:24,authors:"T. Green, M. Gubri, H. Puerto, S. Yun, and S. J. Oh",title:"Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",year:"Jun. 2025",type:"conference",url:"http://arxiv.org/pdf/2506.15674v1"},{number:25,authors:"S. Pahune, Z. Akhtar, V. Mandapati, and K. Siddique",title:"The Importance of AI Data Governance in Large Language Models",venue:"BDCC",volume:"9",pages:"147",year:"2025",doi:"10.3390/bdcc9060147",type:"journal"},{number:26,authors:"V. C. M\xfcller",title:"Ethics of Artificial Intelligence and Robotics",url:"https://plato.stanford.edu/entries/ethics-ai/",accessDate:"Jun. 17 2025",type:"web"},{number:27,authors:"European Union",title:"Proposal for a Regulation laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act)",year:"2021",url:"https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206",accessDate:"Jun. 17 2025",type:"web"},{number:28,authors:"MineDojo",title:"Voyager FAQ",venue:"GitHub, Inc.",year:"2023",url:"https://github.com/MineDojo/Voyager/blob/main/FAQ.md",accessDate:"Jun. 19 2025",type:"web"},{number:29,authors:"A. Mathews",title:"Cost-benefit analysis: is generative AI worth the investment for enterprises",url:"https://aimresearch.co/conference-videos/cost-benefit-analysis-is-generative-ai-worth-the-investment-for-enterprises",accessDate:"Jun. 17 2025",type:"web"},{number:30,authors:"M. Steinberg",title:"Cost-benefit analysis of AI technology investments in Finance",url:"https://www.mindbridge.ai/blog/cost-benefit-analysis-of-ai-technology-investments-in-finance/",accessDate:"Jun. 17 2025",type:"web"},{number:31,authors:"M. Z. Elbashir, P. A. Collier, and M. J. Davern",title:"Measuring the effects of business intelligence systems: The relationship between business process and organizational performance",venue:"International Journal of Accounting Information Systems",volume:"9",pages:"135\u2013153",year:"2008",doi:"10.1016/j.accinf.2008.03.001",type:"journal"},{number:32,authors:"J. Li et al.",title:"Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time",year:"Feb. 2025",type:"conference",url:"http://arxiv.org/pdf/2502.19230v1"},{number:33,authors:"X. Wu",title:"Socratic RL: A Novel Framework for Efficient Knowledge Acquisition through Iterative Reflection and Viewpoint Distillation",year:"Jun. 2025",type:"conference",url:"http://arxiv.org/pdf/2506.13358v1"},{number:34,authors:"K. J. L. Koa, Y. Ma, R. Ng, and T.-S. Chua",title:"Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models",volume:"12706",pages:"4304\u20134315",year:"2024",doi:"10.1145/3589334.3645611",type:"journal"},{number:35,authors:"N. Potamitis and A. Arora",title:"Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback",year:"Apr. 2025",type:"conference",url:"http://arxiv.org/pdf/2504.12951v1"}]}ngOnInit(){this.navigationService.setCurrentSection("bibliography"),this.updateCategoryCounts()}selectCategory(t){this.selectedCategory=t}getFilteredReferences(){return"all"===this.selectedCategory?this.references:this.references.filter(t=>t.type===this.selectedCategory)}getCategoryName(t){return{journal:"Journal",conference:"Conference",web:"Web Resource",book:"Book"}[t]||"Other"}getJournalCount(){return this.references.filter(t=>"journal"===t.type).length}getConferenceCount(){return this.references.filter(t=>"conference"===t.type).length}getWebResourceCount(){return this.references.filter(t=>"web"===t.type).length}updateCategoryCounts(){this.categories=[{key:"all",name:"All References",icon:"fas fa-list",count:this.references.length},{key:"journal",name:"Journal Articles",icon:"fas fa-file-alt",count:this.getJournalCount()},{key:"conference",name:"Conference Papers",icon:"fas fa-users",count:this.getConferenceCount()},{key:"web",name:"Web Resources",icon:"fas fa-globe",count:this.getWebResourceCount()},{key:"book",name:"Books",icon:"fas fa-book",count:this.references.filter(t=>"book"===t.type).length}]}viewOriginalPaper(){window.open("assets/documents/Reflexion and Beyond Verbal Self-Reflection in Large Language Models.pdf","_blank","width=1200,height=800,scrollbars=yes,resizable=yes")}downloadOriginalPaper(){const r=document.createElement("a");r.href="assets/documents/Reflexion and Beyond Verbal Self-Reflection in Large Language Models.pdf",r.download="Reflexion and Beyond Verbal Self-Reflection in Large Language Models.pdf",document.body.appendChild(r),r.click(),document.body.removeChild(r)}static{this.\u0275fac=function(r){return new(r||n)(e.rXU(u.o))}}static{this.\u0275cmp=e.VBU({type:n,selectors:[["app-bibliography"]],standalone:!0,features:[e.aNF],decls:107,vars:9,consts:[[1,"bibliography-container","animate-fade-in"],[1,"page-header","mb-8"],[1,"flex","items-center","justify-between","mb-6"],[1,"text-4xl","font-bold","text-gray-900","dark:text-white","mb-2"],[1,"fas","fa-book","mr-3","text-lavender-600"],[1,"text-xl","text-gray-600","dark:text-gray-400"],[1,"hidden","md:block"],[1,"w-20","h-20","bg-gradient-to-br","from-indigo-500","to-purple-600","rounded-full","flex","items-center","justify-center"],[1,"fas","fa-quote-left","text-white","text-2xl"],[1,"content-section","mb-12"],[1,"card"],[1,"flex","items-start","space-x-4","mb-6"],[1,"w-12","h-12","bg-blue-100","dark:bg-blue-900/20","rounded-lg","flex","items-center","justify-center","flex-shrink-0"],[1,"fas","fa-info-circle","text-blue-600","dark:text-blue-400"],[1,"text-2xl","font-bold","text-gray-900","dark:text-white","mb-2"],[1,"text-gray-600","dark:text-gray-400"],[1,"bg-gradient-to-r","from-blue-50","to-purple-50","dark:from-blue-900/20","dark:to-purple-900/20","rounded-lg","p-6","border","border-blue-200","dark:border-blue-800"],[1,"text-gray-700","dark:text-gray-300","leading-relaxed","mb-4"],[1,"grid","grid-cols-1","md:grid-cols-3","gap-4","text-center"],[1,"bg-white","dark:bg-gray-800","rounded-lg","p-4"],[1,"text-2xl","font-bold","text-blue-600","mb-1"],[1,"text-sm","text-gray-600","dark:text-gray-400"],[1,"text-2xl","font-bold","text-green-600","mb-1"],[1,"text-2xl","font-bold","text-purple-600","mb-1"],[1,"content-section","mb-8"],[1,"text-lg","font-semibold","text-gray-900","dark:text-white","mb-4"],[1,"fas","fa-filter","mr-2","text-gray-600"],[1,"flex","flex-wrap","gap-2"],["class","px-4 py-2 rounded-full text-sm font-medium transition-all duration-200",3,"ngClass","click",4,"ngFor","ngForOf"],[1,"text-2xl","font-bold","text-gray-900","dark:text-white"],[1,"text-lg","font-normal","text-gray-600","dark:text-gray-400","ml-2"],["class","btn-secondary text-sm",3,"click",4,"ngIf"],[1,"space-y-6"],["class","reference-item bg-gray-50 dark:bg-dark-secondary rounded-lg p-6 border border-gray-200 dark:border-gray-700 hover:border-blue-300 dark:hover:border-blue-600 transition-all duration-200",4,"ngFor","ngForOf"],[1,"w-12","h-12","bg-green-100","dark:bg-green-900/20","rounded-lg","flex","items-center","justify-center","flex-shrink-0"],[1,"fas","fa-graduation-cap","text-green-600","dark:text-green-400"],[1,"bg-gradient-to-r","from-blue-50","to-indigo-50","dark:from-blue-900/20","dark:to-indigo-900/20","rounded-lg","p-6","border","border-blue-200","dark:border-blue-800","mb-6"],[1,"flex","items-center","justify-between"],[1,"font-semibold","text-gray-900","dark:text-white","mb-2"],[1,"fas","fa-file-pdf","mr-2","text-blue-600"],[1,"text-gray-600","dark:text-gray-400","text-sm"],[1,"flex","space-x-3"],[1,"btn-primary","text-sm",3,"click"],[1,"fas","fa-eye","mr-2"],[1,"btn-secondary","text-sm",3,"click"],[1,"fas","fa-download","mr-2"],[1,"bg-gradient-to-r","from-green-50","to-blue-50","dark:from-green-900/20","dark:to-blue-900/20","rounded-lg","p-6","border","border-green-200","dark:border-green-800"],[1,"font-semibold","text-gray-900","dark:text-white","mb-4"],[1,"text-gray-700","dark:text-gray-300","mb-4","leading-relaxed"],[1,"bg-white","dark:bg-gray-800","rounded-lg","p-4","border","border-gray-200","dark:border-gray-700"],[1,"font-medium","text-gray-900","dark:text-white","mb-2"],[1,"text-sm","text-gray-600","dark:text-gray-400","font-mono"],[1,"navigation-section"],[1,"flex","justify-between","items-center"],["routerLink","/progress",1,"btn-secondary"],[1,"fas","fa-arrow-left","mr-2"],["routerLink","/home",1,"btn-primary"],[1,"fas","fa-home","ml-2"],[1,"px-4","py-2","rounded-full","text-sm","font-medium","transition-all","duration-200",3,"click","ngClass"],[1,"fas","fa-times","mr-2"],[1,"reference-item","bg-gray-50","dark:bg-dark-secondary","rounded-lg","p-6","border","border-gray-200","dark:border-gray-700","hover:border-blue-300","dark:hover:border-blue-600","transition-all","duration-200"],[1,"flex","items-start","justify-between","mb-3"],[1,"inline-flex","items-center","px-3","py-1","rounded-full","text-sm","font-medium","bg-blue-100","text-blue-800","dark:bg-blue-900/20","dark:text-blue-400"],[1,"inline-flex","items-center","px-2","py-1","rounded-full","text-xs","font-medium",3,"ngClass"],[1,"mb-2"],[1,"font-medium","text-gray-900","dark:text-white"],[1,"text-lg","font-semibold","text-gray-800","dark:text-gray-200"],[1,"mb-3","text-gray-600","dark:text-gray-400"],["class","italic",4,"ngIf"],["class","ml-2",4,"ngIf"],["class","mb-3",4,"ngIf"],["class","text-xs text-gray-500 dark:text-gray-500",4,"ngIf"],[1,"italic"],[1,"ml-2"],[1,"mb-3"],["target","_blank",1,"inline-flex","items-center","text-blue-600","dark:text-blue-400","hover:text-blue-800","dark:hover:text-blue-300","text-sm",3,"href"],[1,"fas","fa-external-link-alt","mr-2"],[1,"text-xs","text-gray-500","dark:text-gray-500"]],template:function(r,o){1&r&&(e.j41(0,"div",0)(1,"section",1)(2,"div",2)(3,"div")(4,"h1",3),e.nrm(5,"i",4),e.EFF(6," Bibliography "),e.k0s(),e.j41(7,"p",5),e.EFF(8," Academic sources and references for LLM agent research "),e.k0s()(),e.j41(9,"div",6)(10,"div",7),e.nrm(11,"i",8),e.k0s()()()(),e.j41(12,"section",9)(13,"div",10)(14,"div",11)(15,"div",12),e.nrm(16,"i",13),e.k0s(),e.j41(17,"div")(18,"h2",14),e.EFF(19," Research Foundation "),e.k0s(),e.j41(20,"p",15),e.EFF(21," This learning hub is built upon cutting-edge research in LLM agents and verbal self-reflection "),e.k0s()()(),e.j41(22,"div",16)(23,"p",17),e.EFF(24," The content presented in this educational platform draws from a comprehensive collection of "),e.j41(25,"strong"),e.EFF(26),e.k0s(),e.EFF(27,", spanning foundational reinforcement learning theory to the latest developments in self-reflective AI systems. "),e.k0s(),e.j41(28,"div",18)(29,"div",19)(30,"div",20),e.EFF(31),e.k0s(),e.j41(32,"div",21),e.EFF(33,"Journal Articles"),e.k0s()(),e.j41(34,"div",19)(35,"div",22),e.EFF(36),e.k0s(),e.j41(37,"div",21),e.EFF(38,"Conference Papers"),e.k0s()(),e.j41(39,"div",19)(40,"div",23),e.EFF(41),e.k0s(),e.j41(42,"div",21),e.EFF(43,"Web Resources"),e.k0s()()()()()(),e.j41(44,"section",24)(45,"div",10)(46,"h3",25),e.nrm(47,"i",26),e.EFF(48," Filter by Category "),e.k0s(),e.j41(49,"div",27),e.DNE(50,p,3,8,"button",28),e.k0s()()(),e.j41(51,"section",9)(52,"div",10)(53,"div",2)(54,"h2",29),e.EFF(55," References "),e.j41(56,"span",30),e.EFF(57),e.k0s()(),e.DNE(58,m,3,0,"button",31),e.k0s(),e.j41(59,"div",32),e.DNE(60,_,20,17,"div",33),e.k0s()()(),e.j41(61,"section",9)(62,"div",10)(63,"div",11)(64,"div",34),e.nrm(65,"i",35),e.k0s(),e.j41(66,"div")(67,"h2",14),e.EFF(68," Citation Guidelines "),e.k0s(),e.j41(69,"p",15),e.EFF(70," Proper academic citation formats for referencing these sources "),e.k0s()()(),e.j41(71,"div",36)(72,"div",37)(73,"div")(74,"h3",38),e.nrm(75,"i",39),e.EFF(76," Access Original Research Paper "),e.k0s(),e.j41(77,"p",40),e.EFF(78," View the foundational paper that this learning hub is based on "),e.k0s()(),e.j41(79,"div",41)(80,"button",42),e.bIt("click",function(){return o.viewOriginalPaper()}),e.nrm(81,"i",43),e.EFF(82," View PDF "),e.k0s(),e.j41(83,"button",44),e.bIt("click",function(){return o.downloadOriginalPaper()}),e.nrm(84,"i",45),e.EFF(85," Download "),e.k0s()()()(),e.j41(86,"div",46)(87,"h3",47),e.EFF(88,"IEEE Citation Format"),e.k0s(),e.j41(89,"p",48),e.EFF(90," The references in this bibliography follow the "),e.j41(91,"strong"),e.EFF(92,"IEEE citation format"),e.k0s(),e.EFF(93,", which is commonly used in computer science and engineering research. This format provides a standardized way to cite academic sources and enables proper attribution of research contributions. "),e.k0s(),e.j41(94,"div",49)(95,"h4",50),e.EFF(96,"Example Citation:"),e.k0s(),e.j41(97,"p",51),e.EFF(98,' [2] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao, "Reflexion: Language Agents with Verbal Reinforcement Learning," Mar. 2023. [Online]. Available: http://arxiv.org/pdf/2303.11366v4 '),e.k0s()()()()(),e.j41(99,"section",52)(100,"div",53)(101,"button",54),e.nrm(102,"i",55),e.EFF(103," Back to Progress "),e.k0s(),e.j41(104,"button",56),e.EFF(105," Return to Home "),e.nrm(106,"i",57),e.k0s()()()()),2&r&&(e.R7$(26),e.SpI("",o.references.length," peer-reviewed academic sources"),e.R7$(5),e.JRh(o.getJournalCount()),e.R7$(5),e.JRh(o.getConferenceCount()),e.R7$(5),e.JRh(o.getWebResourceCount()),e.R7$(9),e.Y8G("ngForOf",o.categories),e.R7$(7),e.Lme(" (",o.getFilteredReferences().length," of ",o.references.length,") "),e.R7$(),e.Y8G("ngIf","all"!==o.selectedCategory),e.R7$(2),e.Y8G("ngForOf",o.getFilteredReferences()))},dependencies:[s.MD,s.YU,s.Sq,s.bT,g.iI,g.Wk],styles:[".reference-item[_ngcontent-%COMP%]{transition:all .2s ease-in-out}.reference-item[_ngcontent-%COMP%]:hover{transform:translateY(-1px);box-shadow:0 4px 12px #0000001a}.dark[_ngcontent-%COMP%]   .reference-item[_ngcontent-%COMP%]:hover{box-shadow:0 4px 12px #0000004d}.category-filter[_ngcontent-%COMP%]{transition:all .2s ease-in-out}.citation-example[_ngcontent-%COMP%]{font-family:Monaco,Menlo,Ubuntu Mono,monospace;line-height:1.6}.type-badge[_ngcontent-%COMP%]{font-size:.75rem;font-weight:600;text-transform:uppercase;letter-spacing:.025em}@media (max-width: 768px){.reference-item[_ngcontent-%COMP%]{padding:1rem}.categories-grid[_ngcontent-%COMP%]{grid-template-columns:1fr}}"]})}}return n})()}}]);